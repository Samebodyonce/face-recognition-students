{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mtcnn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imports\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom PIL import Image\n\n# Confirm mtcnn was installed correctly\nimport mtcnn\nfrom mtcnn.mtcnn import MTCNN\nfrom matplotlib.patches import Rectangle\n\nfrom os import listdir\nfrom tqdm import tqdm\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DIRECTORY = \"../input/yale-face-database/\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis\nThe Yale Face Database is made from 165 grayscale images of 15 people. There are 11 images per person with different expressions. ","metadata":{}},{"cell_type":"code","source":"filename = \"../input/yale-face-database/subject01.centerlight\"\npixels = plt.imread(filename)\n\nrgb_pixels = np.stack((pixels, pixels, pixels), axis=2)\nprint(rgb_pixels.shape)\nplt.imshow(pixels)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In each of the images, the face is positioned in a different region. The first task is to normalise all of the face positions so that they can reliably be fed into a classifier. There are are few techniques to achieve this. Haar Cascades provide a simple and fast method for detecting faces but they can be unreliable. They are ideally suited to real time detection. Facial identification can afford to spend longer processing the image. Therefore we will use MTCNN which is a face detection algorithm using a CNN, it achieves much higher accuracy than other techniques.","metadata":{}},{"cell_type":"code","source":"# Create the detector, using default weights\ndetector = MTCNN()\n# detect faces in the image\nresults = detector.detect_faces(rgb_pixels)\nresults","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# draw an image with detected objects\ndef draw_image_with_boxes(data, result_list):\n    # plot the image\n    plt.imshow(data)\n    # get the context for drawing boxes\n    ax = plt.gca()\n    # plot each box\n    for result in result_list:\n        # get coordinates\n        x, y, width, height = result['box']\n        # create the shape\n        rect = Rectangle((x, y), width, height, fill=False, color='red')\n        # draw the box\n        ax.add_patch(rect)\n    # show the plot\n    plt.show()\n\n# display faces on the original image\ndraw_image_with_boxes(rgb_pixels, results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next step is to extract and normalise the face pixels so that they can reliably be used for classifying.","metadata":{}},{"cell_type":"code","source":"# extract a single face from a given photograph\ndef extract_face_from_file(filename, required_size=(160, 160)):\n    # load image from file\n    image = Image.open(filename)\n    \n    return extract_face(image, required_size)\n\ndef extract_face(image, required_size=(160, 160)):\n    # convert to RGB, if needed\n    image = image.convert('RGB')\n    # convert to array\n    pixels = np.asarray(image)\n    # detect faces in the image\n    results = detector.detect_faces(pixels)\n    # extract the bounding box from the first face\n    x1, y1, width, height = results[0]['box']\n    # bug fix\n    x1, y1 = abs(x1), abs(y1)\n    x2, y2 = x1 + width, y1 + height\n    # extract the face\n    face = pixels[y1:y2, x1:x2]\n    # resize pixels to the model size\n    image = Image.fromarray(face)\n    image = image.resize(required_size)\n    face_array = np.asarray(image)\n    gray_face = cv2.cvtColor(face_array, cv2.COLOR_BGR2GRAY)\n    \n    return gray_face\n\n\n# Create the detector, using default weights\ndetector = MTCNN()\n\n# load the photo and extract the face\nface_pixels = extract_face_from_file(\"../input/yale-face-database/subject01.centerlight\")\n\nplt.imshow(face_pixels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the dataset\nWe need to extract the faces for all of the images so that we can create our dataset for training/testing.","metadata":{}},{"cell_type":"code","source":"def list_files(directory, contains):\n    return list(f for f in listdir(directory) if contains in f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 1\nfaces = list()\nfor filename in tqdm(list_files(DIRECTORY, \"subject\")[0:16]):\n    # path\n    path = DIRECTORY + filename\n    # get face\n    face = extract_face_from_file(path)\n    # plot\n    plt.subplot(4, 4, i)\n    plt.axis('off')\n    plt.imshow(face)\n    faces.append(face)\n    i += 1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup the test train data","metadata":{}},{"cell_type":"code","source":"# list filenames\nfilenames = pd.DataFrame(list_files(DIRECTORY, \"subject\"))\n\n# generate split \ndf = filenames[0].str.split(\".\", expand=True)\ndf[\"filename\"] = filenames\n\n# # tidy columns\ndf = df.rename(columns = {0:\"subject\", 1:\"category\"})\ndf['subject'] = df.subject.str.replace('subject' , '')\ndf.apply(pd.to_numeric, errors='coerce').dropna()\ndf['subject'] = pd.to_numeric(df[\"subject\"])\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print classnames\ndf['subject'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test Train Split\n\nThere are a limited number of samples per class, although we need to a suitable number of classes in order to benchmark the models appropriately. I decided on a test train split of ~ 70/30 (enough for 3 test images).","metadata":{}},{"cell_type":"code","source":"PER_CLASS = 8 # 11 images (3 test & 8 train)\nNO_CLASSES = 15\nDS_SIZE = df[\"subject\"].count()\nTEST_SIZE = 1 - (PER_CLASS * NO_CLASSES / DS_SIZE)\n\n# # list files for each group\n# # df.groupby(['subject'])['filename'].apply(list)\ny = df['subject']\nX = df.drop('subject',axis=1)\n\n# # subject\nX_train_info, X_test_info, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=45, stratify=y)\n\ny_train = y_train.tolist()\ny_test = y_test.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extract the faces","metadata":{}},{"cell_type":"code","source":"detector = MTCNN()\n\ndef load_dataset(dataset):\n    faces = list()\n    for filename in tqdm(dataset[\"filename\"]):\n        path = DIRECTORY + filename\n        # get face\n        face = extract_face_from_file(path)\n        faces.append(face)\n    return np.asarray(faces)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = load_dataset(X_test_info)\nX_train = load_dataset(X_train_info)\n\nprint(X_test.shape)\nprint(X_train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the SVM Model\n\nAn SVM model gives us a good baseline as to the performance of the dataset that we can expect","metadata":{}},{"cell_type":"code","source":"# develop a classifier for faces\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport random","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize input vectors\nin_encoder = Normalizer(norm='l2')\n\nX_train_reshaped = X_train.reshape(X_train.shape[0],X_train.shape[1]*X_train.shape[2])\nprint('Reshaped X_train', X_train_reshaped.shape)\nX_train_reshaped = in_encoder.transform(X_train_reshaped)\n\nX_test_reshaped = X_test.reshape(X_test.shape[0],X_test.shape[1]*X_test.shape[2])\nprint('Reshaped X_test', X_test_reshaped.shape)\nX_test_reshaped = in_encoder.transform(X_test_reshaped)\n\n# Label encode targets\nout_encoder = LabelEncoder()\nout_encoder.fit(y_train)\n\n# Fit model\nmodel = SVC(kernel='linear', probability=True)\nmodel.fit(X_train_reshaped, y_train)\n\n# Predict\nyhat_train = model.predict(X_train_reshaped)\nyhat_test = model.predict(X_test_reshaped)\n\n# Score\nscore_train = accuracy_score(y_train, yhat_train)\nscore_test = accuracy_score(y_test, yhat_test)\n# Summarize\nprint('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_predictions = model.predict(X_test_reshaped)\nprint(classification_report(y_test,svm_predictions))\n\n# Display the confusion matrix:\n# [TP,FP]\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test,svm_predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The result achieves 75% accuracy on the test set which is a fairly good result, testing other models will give us a better benchmark.","metadata":{}},{"cell_type":"markdown","source":"## Random Sample testing","metadata":{}},{"cell_type":"code","source":"# Test model on a random example from the test dataset\nselection = random.randint(1, X_test.shape[0])\nrandom_face_emb = X_test_reshaped[selection]\nrandom_face_class = y_test[selection]\nrandom_face_name = out_encoder.inverse_transform([random_face_class]) - 1\n\n# prediction for the face\nsamples = np.expand_dims(random_face_emb, axis=0)\nyhat_class = model.predict(samples)\nyhat_prob = model.predict_proba(samples)\n\n# get name\nclass_index = yhat_class[0]\n\nclass_probability = yhat_prob[0,class_index] * 100\npredict_names = out_encoder.inverse_transform(yhat_class) - 1\nprint('Predicted: %s (%.3f)' % (predict_names[0], class_probability))\nprint('Expected: %s' % random_face_name[0])\n\n# Show the image\nplt.imshow(X_train[selection])\ntitle = 'Predicted: %s (%.3f)' % (predict_names[0], class_probability)\nplt.title(title)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Random Forrest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train_reshaped, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_predictions = rfc.predict(X_test_reshaped)\nprint(classification_report(y_test,rf_predictions))\n\n# Display the confusion matrix:\n# [TP,FP]\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test,rf_predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test model on a random example from the test dataset\nselection = random.randint(1, X_test.shape[0])\nrandom_face_emb = X_test_reshaped[selection]\nrandom_face_class = y_test[selection]\nrandom_face_name = out_encoder.inverse_transform([random_face_class]) - 1\n\n# Prediction for the face\nsamples = np.expand_dims(random_face_emb, axis=0)\nyhat_class = rfc.predict(samples)\nyhat_prob = rfc.predict_proba(samples)\n\n# Get name\nclass_index = yhat_class[0]\n\n# Calculate results\nclass_probability = yhat_prob[0,class_index] * 100\npredict_names = out_encoder.inverse_transform(yhat_class) - 1\nprint('Predicted: %s (%.3f)' % (predict_names[0], class_probability))\nprint('Expected: %s' % random_face_name[0])\n\n# Show the image\nplt.imshow(X_test[selection])\ntitle = 'Predicted: %s (%.3f)' % (predict_names[0], class_probability)\nplt.title(title)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training a Convolutional Neural Network","metadata":{}},{"cell_type":"code","source":"# Options \n\nTRAINING_DATA_DIRECTORY = \"data/train\"\nTESTING_DATA_DIRECTORY = \"data/test\"\nNUM_CLASSES = 15\nEPOCHS = 25\nBATCH_SIZE = 20\nNUMBER_OF_TRAINING_IMAGES = 120\nNUMBER_OF_TESTING_IMAGES = 45\nIMAGE_HEIGHT = 160\nIMAGE_WIDTH = 160","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save the dataset to disk to load into keras\n\nKeras has built in features to load data from disk, so we will save the dataset.","metadata":{}},{"cell_type":"code","source":"import os \n\ndef save_keras_dataset(setname, dataset, labels, per_class):\n    # combine labels and images to generate files\n    data = sorted(list(zip(labels, dataset)), key=lambda x: x[0])\n\n    # Save images\n    j = 0\n    for label, gray_img in tqdm(data):\n        j = (j% per_class) + 1\n        # Create directory\n        directory = f\"data/{setname}/class_{label}/\"\n        if not os.path.exists(directory):\n                os.makedirs(directory)\n        cv2.imwrite(f\"{directory}class_{label}_{j}.png\",gray_img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clear directory if it already exists\nimport shutil\nshutil.rmtree(r'data', ignore_errors=True)\n\n# Save datasets\nsave_keras_dataset(\"test\", X_test, y_test, 3)\nsave_keras_dataset(\"train\", X_train, y_train, 8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configure data generators with random gaussian noise, zooming and rotation \n\nKeras includes an ImageDataGenerator which can automatically augment the dataset.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ndef data_generator():\n    return ImageDataGenerator(\n        rescale=1./255,\n        # horizontal_flip=True,\n#         fill_mode=\"nearest\",\n#         zoom_range=0.1,\n#         width_shift_range=0.1,\n#         height_shift_range=0.1,\n#         rotation_range=10,\n        # preprocessing_function=add_noise\n    )\n\ndef add_noise(img):\n    \"\"\"Add random noise to an image\"\"\"\n    VARIABILITY = 35\n    deviation = VARIABILITY*random.random()\n    noise = np.random.normal(0, deviation, img.shape)\n    img += noise\n    np.clip(img, 0., 255.)\n    return img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setup Data Generators\ntraining_generator = data_generator().flow_from_directory(\n    TRAINING_DATA_DIRECTORY,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    color_mode='grayscale'\n)\n\ntesting_generator = data_generator().flow_from_directory(\n    TESTING_DATA_DIRECTORY,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    class_mode='categorical',\n    color_mode='grayscale'\n)\n\nvalidation_generator = data_generator().flow_from_directory(\n    TESTING_DATA_DIRECTORY,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    class_mode='categorical',\n    color_mode='grayscale',\n    shuffle=False # IMPORTANT: to ensure classes line up with batches\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_images = testing_generator.next()[0]\n\nf, xyarr = plt.subplots(3,3)\nxyarr[0,0].imshow(sample_images[0])\nxyarr[0,1].imshow(sample_images[1])\nxyarr[0,2].imshow(sample_images[2])\nxyarr[1,0].imshow(sample_images[3])\nxyarr[1,1].imshow(sample_images[4])\nxyarr[1,2].imshow(sample_images[5])\nxyarr[2,0].imshow(sample_images[6])\nxyarr[2,1].imshow(sample_images[7])\nxyarr[2,2].imshow(sample_images[8])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nclass MCDropout(keras.layers.Dropout):\n    def call(self, inputs):\n        return super().call(inputs, training=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import models\nfrom tensorflow.keras.layers import Activation, ZeroPadding2D, MaxPooling2D, Conv2D, Flatten, Dense, Dropout\nfrom tensorflow.keras import regularizers, constraints\n\n# Define a sequential keras model\nmodel = models.Sequential()\n\n# 1st Convolution layer\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='linear', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 1), padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\n# 2nd Convolution layer\nmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(l2=0.01)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# 3rd Convolution layer\nmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(l2=0.01)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Flatten the Convolution\nmodel.add(Flatten())\n\n# Define a dense layer with l2 regularizer to reduce overfitting\nmodel.add(Dense(512, activation='relu', kernel_initializer=\"glorot_uniform\", kernel_regularizer=regularizers.l2(l2=0.01)))\n\n# Define a drop layer to reduce overfitting\nmodel.add(MCDropout(rate=0.5))\n\n# Final output layer\nmodel.add(Dense(NUM_CLASSES, activation='softmax', kernel_initializer=\"glorot_uniform\"))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(model, show_shapes=True, show_layer_names=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import optimizers, losses\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping()\n\nmodel.compile(\n    loss=losses.CategoricalCrossentropy(from_logits=True),\n    optimizer=optimizers.Adam(learning_rate=0.0003),\n    metrics=[\"accuracy\"]\n)\n\nhistory = model.fit(\n    training_generator,\n    steps_per_epoch=(NUMBER_OF_TRAINING_IMAGES//BATCH_SIZE ),\n    epochs=EPOCHS,\n    validation_data=testing_generator,\n    shuffle=True,\n    validation_steps=(NUMBER_OF_TESTING_IMAGES//BATCH_SIZE),\n#     callbacks=[early_stopping]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CNN Model Evaluation\n\nAfter training, we can evaluate the model to test its accuracy.\n\nWe need to test for overfitting which occurs when the accuracy of the training set is sigificantly higher than the testing set due to the loss function.\n\nImplementing l2 regularization attempts to reduce overfitting by penalizing large weights in the network. Due to the small size of the dataset, there has to be a trade off between overfitting and accuracy.","metadata":{}},{"cell_type":"code","source":"plot_folder = \"plot\"\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.1, 1])\nplt.legend(loc='lower right')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_folder = \"plot\"\nplt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Val Loss')\nplt.legend(loc='lower right')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n\nY_pred = model.predict(validation_generator)\ny_pred = np.argmax(Y_pred, axis=1)\nprint(classification_report(validation_generator.classes, y_pred))\nprint(validation_generator.classes)\nprint(y_pred)\nprint('Confusion Matrix')\nprint(confusion_matrix(validation_generator.classes, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save the model\n\nThe model can be saved for future processing","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"keras_face_recognition.h5\"\nmodel_path = \"./model\"\nif not os.path.exists(model_path):\n    os.mkdir(model_path)\n\nmodel.save(os.path.join(model_path, MODEL_NAME))\nclass_names = training_generator.class_indices\nclass_names_file_reverse = MODEL_NAME[:-3] + \"_class_names_reverse.npy\"\nclass_names_file = MODEL_NAME[:-3] + \"_class_names.npy\"\nnp.save(os.path.join(model_path, class_names_file_reverse), class_names)\nclass_names_reversed = np.load(os.path.join(model_path, class_names_file_reverse), allow_pickle=True).item()\nclass_names = dict([(value, key) for key, value in class_names_reversed.items()])\nnp.save(os.path.join(model_path, class_names_file), class_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sample Testing","metadata":{}},{"cell_type":"code","source":"def get_sample_test_image():\n    \"\"\"Chooses a random image from the testing set\"\"\"\n    \n    # Choose image sample\n    expected_class = random.randint(1, NUM_CLASSES)\n    random_sample = random.randint(1, 3)\n\n    # Build image path\n    image_path = f\"data/train/class_{expected_class}/class_{expected_class}_{random_sample}.png\"\n\n    # Read the file\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n    # Return the results\n    return img, expected_class","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_image(img):\n    \"\"\"Ensures the image is the correct shape and normalises the pixels\"\"\"\n\n    image = Image.fromarray(img)\n    image = image.resize((160,160))\n    face_array = np.asarray(image)\n    # expands the dimensions\n    face_array = face_array.reshape(160,160,1)\n\n    face_array = face_array.astype('float32')\n    scaled_image = np.expand_dims(face_array, axis=0)\n\n    return scaled_image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prediction(image, debug=True): \n    # show the image\n    plt.imshow(image)\n    plt.show()\n\n    # Process the sample\n    input_sample = preprocess_image(img) \n\n    # Prediction\n    results = model.predict(input_sample)\n    result = np.argmax(results, axis=1)\n    index = result[0]\n\n    # Calculate Confidence\n    confidence = results[0][index] * 100\n    classes = np.load(os.path.join(\"model\", class_names_file), allow_pickle=True).item()\n    # Get class name\n    if type(classes) is dict:\n        for k, v in classes.items():\n            if k == index:\n                class_name = v\n    if debug:\n        print(results)\n        print(\"Detected class is {} with {:.2f}% confidence\".format(class_name, round(confidence, 2)))\n    \n    # Return results\n    return class_name, confidence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose an image\nimg, expected_class = get_sample_test_image()\n\nprint(f\"Expected class: {expected_class}\")\n\nprediction(img), f\"expected:{expected_class}\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Monte Carlo Dropout","metadata":{}},{"cell_type":"code","source":"def monte_carlo_prediction(image, debug=True): \n    # show the image\n    plt.imshow(image)\n    plt.show()\n    \n    # Process the sample\n    input_sample = preprocess_image(img)\n\n    # Prediction\n    results = np.stack([model(input_sample, training=True) for _ in range(100)])\n\n    # Calucate Results\n    results_mean = results.mean(axis=0)\n    results_std = results.std(axis=0)\n    index = np.argmax(results_mean,axis=1)\n\n    # Calculate Confidence\n    confidence = results_mean[0][index][0] * 100\n\n    classes = np.load(os.path.join(\"model\", class_names_file), allow_pickle=True).item()\n    if type(classes) is dict:\n        for k, v in classes.items():\n            if k == index:\n                class_name = v\n\n    if (debug):\n        print(f'Mean = {np.round(results_mean[:1],2)}')\n        print(f'Std  = {np.round(results_std[:1],2)}')\n        print(confidence)\n        print(\"detected class is {} with {:.2f}% confidence\".format(class_name, round(confidence, 2)))\n        \n    return class_name, confidence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose an image\nimg, expected_class = get_sample_test_image()\nprint(f\"expected class {expected_class}\")\n\nmonte_carlo_prediction(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the authentication mechanism\n\nWe can filter against this by ensuring that a person is not allowed into the building if the classification is below a certain threshold. ","metadata":{}},{"cell_type":"code","source":"THRESHOLD = 80\n\nALLOWED_USERS = [\"class_01\", \"class_03\", \"class_05\", \"class_07\", \"class_09\", \"class_11\", \"class_13\", \"class_15\"]\n\ndef authenticate(img, debug=False):\n    classname, confidence = monte_carlo_prediction(img, debug)\n\n    if (confidence < THRESHOLD):\n        # Not authenticated\n        print(\"Face not recognised\")\n    elif (classname in ALLOWED_USERS):\n        print(\"Welcome {}\".format(classname))\n    else:\n        print(\"You are not permitted {}\".format(classname))\n\n    return classname, confidence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, expected_class = get_sample_test_image()\nprint(f\"expected class {expected_class}\")\n\nauthenticate(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}